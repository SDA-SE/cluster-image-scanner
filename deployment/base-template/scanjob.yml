kind: WorkflowTemplate
metadata:
  name: scan-image-job
  namespace: clusterscanner
spec:
  activeDeadlineSeconds: 3600
  artifactRepositoryRef:
    configMap: artifact-repositories
    key: default-v1
  ttlStrategy:
    secondsAfterSuccess: 3600
    secondsAfterFailure: 86400
  entrypoint: main # Entry point for job execution
  inputs:
    parameters:
      - name: REGISTRY_SECRET
      - name: DEPENDENCY_SCAN_CM
      - name: DEFECTDOJO_CM
      - name: DEFECTDOJO_SECRETS
      - name: SCAN_ID
      - name: team
      - name: appname
      - name: environment
      - name: namespace
      - name: scm_source_branch
      - name: image
      - name: image_id
      - name: appversion
      - name: appname
      - name: slack
      - name: is_scan_lifetime
      - name: is_scan_baseimage_lifetime
      - name: is_scan_distroless
      - name: is_scan_malware
      - name: is_scan_dependency_check
      - name: is_scan_runasroot
      - name: scan_lifetime_max_days
      - name: is_scan_new_version
      - name: is_scan_dependency_track
      - name: new_version_image_filter
      - name: dependencyCheckSuppressionsConfigMapName
      - name: baseImageName
      - name: defectDojoClientImageName
      - name: scanDistrolessImageName
      - name: scanDependencyCheckImageName
      - name: scanMalwareImageName
      - name: scanRootImageName
      - name: scanLifetimeImageName
      - name: scanNewVersionImageName
      - name: scanSyftImageName
      - name: scanjobEnvParameter

  templates:
    - name: main
      dag:
        tasks:
          - name: imagefetcher
            template: imagefetcher
          - name: sbom-generation
            depends: imagefetcher
            template: sbom-generation
          - name: scan-dependency-track
            depends: sbom-generation
            template: scan-dependency-track
          - name: results-dependency-track-dd-upload
            depends: scan-dependency-track
            template: results-dependency-track-dd-upload
          - name: scan-distroless
            depends: imagefetcher
            template: scan-distroless
          - name: scan-lifetime
            depends: imagefetcher
            template: scan-lifetime
          - name: scan-baseimage-lifetime
            depends: imagefetcher
            template: scan-baseimage-lifetime
          - name: scan-dependency-check
            depends: imagefetcher
            template: scan-dependency-check
          - name: scan-runasroot
            depends: imagefetcher
            template: scan-runasroot
          - name: scan-malware
            depends: imagefetcher
            template: scan-malware
          - name: scan-new-version
            depends: imagefetcher
            template: scan-new-version
          - name: results-dependency-check-dd-upload
            #when: "{{ workflow.parameters.is_scan_dependency_check }} == true" # v3 allows conditional artifacts, that might be a way
            depends: scan-dependency-check
            template: results-dependency-check-dd-upload
            arguments:
              artifacts:
                - name: results-dependency-check-report
                  from: "{{ tasks.scan-dependency-check.outputs.artifacts.results-dependency-check-report }}"
          - name: results-collect-generic-findings
            depends: (scan-distroless.Succeeded || scan-lifetime.Succeeded || scan-baseimage-lifetime.Succeeded || scan-runasroot.Succeeded || scan-malware.Succeeded || scan-new-version.Succeeded)
            template: results-collect-generic-findings
          - name: results-generic-dd-upload
            depends: results-collect-generic-findings
            template: results-generic-dd-upload
            arguments:
              artifacts:
                - name: results-generic-findings
                  from: "{{ tasks.results-collect-generic-findings.outputs.artifacts.results-generic-findings }}"
          - name: results-aggregate
            depends: (results-dependency-check-dd-upload.Succeeded || results-generic-dd-upload.Succeeded || results-dependency-track-dd-upload.Succeeded)
            template: results-aggregate
            arguments:
              artifacts:
                - name: results-dd-generic-test-link
                  from: "{{ tasks.results-generic-dd-upload.outputs.artifacts.results-dd-generic-test-link }}"
                - name: results-dd-generic-is-finding-file
                  from: "{{ tasks.results-generic-dd-upload.outputs.artifacts.is-finding-file }}"
                - name: results-dd-generic-findings-file
                  from: "{{ tasks.results-generic-dd-upload.outputs.artifacts.findings-file }}"
                - name: results-generic
                  from: "{{ tasks.results-collect-generic-findings.outputs.artifacts.results-generic }}"
                - name: results-dd-dependency-check-test-link
                  from: "{{ tasks.results-dependency-check-dd-upload.outputs.artifacts.results-dd-dependency-check-test-link }}"
                - name: results-dd-dependency-check-is-finding-file
                  from: "{{ tasks.results-dependency-check-dd-upload.outputs.artifacts.is-finding-file }}"
                - name: results-dd-dependency-check-findings-file
                  from: "{{ tasks.results-dependency-check-dd-upload.outputs.artifacts.findings-file }}"
                - name: results-dd-dependency-track-test-link
                  from: "{{ tasks.results-dependency-track-dd-upload.outputs.artifacts.results-dd-dependency-track-test-link }}"
                - name: results-dd-dependency-track-is-finding-file
                  from: "{{ tasks.results-dependency-track-dd-upload.outputs.artifacts.is-finding-file }}"
                - name: results-dd-dependency-track-findings-file
                  from: "{{ tasks.results-dependency-track-dd-upload.outputs.artifacts.findings-file }}"
              parameters:
                - name: "dd-generic-exitcode"
                  value: "{{ tasks.results-generic-dd-upload.exitCode }}"
                - name: "dd-generic-startedat"
                  value: "{{ tasks.results-generic-dd-upload.startedAt }}"
                - name: "dd-generic-finishedat"
                  value: "{{ tasks.results-generic-dd-upload.finishedAt }}"
                - name: "dd-dependency-check-exitcode"
                  value: "{{ tasks.results-dependency-check-dd-upload.exitCode }}"
                - name: "dd-dependency-check-startedat"
                  value: "{{ tasks.results-dependency-check-dd-upload.startedAt }}"
                - name: "dd-dependency-check-finishedat"
                  value: "{{ tasks.results-dependency-check-dd-upload.finishedAt }}"
                - name: "dd-dependency-track-exitcode"
                  value: "{{ tasks.results-dependency-track-dd-upload.exitCode }}"
                - name: "dd-dependency-track-startedat"
                  value: "{{ tasks.results-dependency-track-dd-upload.startedAt }}"
                - name: "dd-dependency-track-finishedat"
                  value: "{{ tasks.results-dependency-track-dd-upload.finishedAt }}"

    - name: imagefetcher
      volumes:
        - name: images
          persistentVolumeClaim:
            claimName: cluster-image-scanner-images
        - name: registry-creds
          secret:
            secretName: "{{ workflow.parameters.REGISTRY_SECRET }}"
      script:
        image: "{{ workflow.parameters.baseImageName }}"
        command: [/bin/bash]
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - name: images
            mountPath: /clusterscanner/images
            subPath: "{{ workflow.parameters.image_id }}"
            defaultMode: 0777 # to allow the cleanup to delete the folders
          - name: registry-creds
            mountPath: /run/containers/auth.json
            subPath: auth.json
            defaultMode: 0444
        env:
          - name: IMAGE
            value: "{{ workflow.parameters.image }}"
          - name: IMAGE_BY_HASH
            value: "{{ workflow.parameters.image_id }}"
        envFrom:
          - configMapRef:
              name: "{{ workflow.parameters.scanjobEnvParameter }}"
        source: |
          # IMAGE_TAR_PATH from base image
          set -e
          echo "fetching image ${IMAGE} with hash ${IMAGE_BY_HASH}"

          IMAGE_TAR_FOLDER_PATH=/clusterscanner/images
          mkdir -p "${IMAGE_TAR_FOLDER_PATH}/" || true
          mkdir -p "${IMAGE_TAR_FOLDER_PATH}/tmp" || true
          tmp_dir=$(mktemp -d --tmpdir="${IMAGE_TAR_FOLDER_PATH}/tmp")
          rm -Rf "${tmp_dir}" || true
          mkdir -p "${tmp_dir}"
          image_copy_dir=$(mktemp -d --tmpdir="${tmp_dir}")
          unpack_dir=$(mktemp -d --tmpdir="${tmp_dir}")
          config_dir=$(mktemp -d --tmpdir="${tmp_dir}")
          mkdir -p "${image_copy_dir}" "${config_dir}" "${unpack_dir}"
          echo "Directory /run/containers"
          ls -lah /run/containers

          echo "Checking for existing config manifest of ${IMAGE_BY_HASH}"
          # catch tagged images (which can happen if we don't know the image hash to pull by). When we have a tagged image, the image is assumed to be mutable - therefore the config.json of the image is checked, to make sure they are identical before skipping the pull process.
          timeout 30 skopeo inspect ${SKOPEO_INSPECT_PARAMETER} --config "docker://${IMAGE_BY_HASH}" | jq -cMS 'def recursively(f): . as $in | if type == "object" then reduce keys_unsorted[] as $key ( {}; . + { ($key):  ($in[$key] | recursively(f)) } ) elif type == "array" then map( recursively(f) ) | f else . end; . | recursively(sort)' > "${config_dir}/config.json"
          ls -la "${config_dir}/config.json"
          if [[ -f "${IMAGE_TAR_FOLDER_PATH}/config.json" ]] && diff -qs "${config_dir}/config.json" "${IMAGE_TAR_FOLDER_PATH}/config.json" && [ -f ${IMAGE_TAR_FOLDER_PATH}/docker-archive-image.tar ]; then
            echo "Already got image with identical config manifest, skipping"
            # touch on folder to modify mtime, so it is not getting cleaned up for MAX_DAYS, see cleanup.yml
            touch "${IMAGE_TAR_PATH}"
            exit 0
          fi

          echo "Downloading image ${IMAGE_BY_HASH} to ${image_copy_dir}"

          skopeo copy ${SKOPEO_COPY_PARAMETER} "docker://${IMAGE_BY_HASH}" "dir:${image_copy_dir}/"
          rm -Rf ${IMAGE_TAR_FOLDER_PATH}/docker-archive-image.tar || true
          skopeo copy ${SKOPEO_COPY_PARAMETER} "docker://${IMAGE_BY_HASH}" "docker-archive:${IMAGE_TAR_FOLDER_PATH}/docker-archive-image.tar"
          layers=$(jq -r '.layers' "${image_copy_dir}/manifest.json")
          if [ "${layers}" != "null" ] && [ "${layers}" != "" ]; then
            extractionCommand=".layers | .[].digest"
          elif [ "$(jq -r '.fsLayers' "${image_copy_dir}/manifest.json")" != "null" ] && [ "$(jq -r '.fsLayers' "${image_copy_dir}/manifest.json")" != "null" ]; then
            extractionCommand=".fsLayers | .[].blobSum"
          else
            echo "Could NOT find layers to unpack"
            exit 2
          fi
          for l in $(jq -r "$extractionCommand" "${image_copy_dir}/manifest.json" | sed -e "s/^sha256://g"); do
              echo "Extracting blob ${l}"
              tar --exclude "/dev/" --no-acls --no-selinux --no-xattrs --no-overwrite-dir --owner=clusterscanner --group=clusterscanner -mxzf "${image_copy_dir}/${l}" -C "${unpack_dir}" || true # TODO scan somehow dev
          done
          echo "Changing directory permissions"
          find "${unpack_dir}" -type d -exec chmod 755 {} +
          echo "Changing file permissions"
          find "${unpack_dir}" -type f -exec chmod 644 {} +
          cd "${unpack_dir}"

          echo "Packing image to ${IMAGE_TAR_FOLDER_PATH}"
          tar -cf "${IMAGE_TAR_PATH}" ./* || exit 1

          echo "Copying manifest and config file"
          cp "${image_copy_dir}/manifest.json" "${IMAGE_TAR_FOLDER_PATH}/manifest.json"
          rm -f "${IMAGE_TAR_FOLDER_PATH}/config.json" || true # in case an other fetcher runs in parallel
          cp "${config_dir}/config.json" "${IMAGE_TAR_FOLDER_PATH}/config.json"

          echo "Cleaning up"
          rm -rf "${tmp_dir}"

    - name: sbom-generation
      volumes:
        - name: images
          persistentVolumeClaim:
            claimName: cluster-image-scanner-images
            readOnly: true
        - name: scandata
          persistentVolumeClaim:
            claimName: cluster-image-scanner-scandata
      container:
        image: "{{ workflow.parameters.scanSyftImageName }}"
        command: ["/clusterscanner/entrypoint.bash"]
        args: [
            "packages",
            "docker-archive:/clusterscanner/images/docker-archive-image.tar",
            "--output",
            "cyclonedx-json",
            "--file",
            "/clusterscanner/data/bom.json"
        ]
        env:
          - name: IS_SCAN
            value: "{{ workflow.parameters.is_scan_dependency_track }}"
        envFrom:
          - configMapRef:
              name: "{{ workflow.parameters.scanjobEnvParameter }}"
        imagePullPolicy: Always
        volumeMounts:
          - name: images
            mountPath: /clusterscanner/images
            subPath: "{{ workflow.parameters.image_id }}"
            readOnly: true
          - name: scandata
            mountPath: /clusterscanner/data
            subPath: "results/{{ workflow.parameters.image_id }}/sbom"

    - name: scan-dependency-track
      volumes:
        - name: scandata
          persistentVolumeClaim:
            claimName: cluster-image-scanner-scandata
      script:
        image: "{{ workflow.parameters.baseImageName }}"
        command: [/bin/bash]
        imagePullPolicy: IfNotPresent
        env:
          - name: IS_SCAN
            value: "{{ workflow.parameters.is_scan_dependency_track }}"
          - name: PROJECT_NAME
            value: "{{ workflow.parameters.environment }} | {{ workflow.parameters.namespace }} | {{ workflow.parameters.appname }}"
          - name: VERSION
            value: "{{ workflow.parameters.appversion }}"
          - name: DD_BRANCH_NAME
            value: "{{ workflow.parameters.image }}"
          - name: NAMESPACE
            value: "{{ workflow.parameters.namespace }}"
          - name: ENVIRONMENT
            value: "{{ workflow.parameters.environment }}"
          - name: TEAM
            value: "{{ workflow.parameters.team }}"
        envFrom:
          - secretRef:
              name: "dependency-track"
          - configMapRef:
              name: "{{ workflow.parameters.scanjobEnvParameter }}"
          - configMapRef:
              name: "dependency-track"
        volumeMounts:
          - name: scandata
            mountPath: /clusterscanner/data
            subPath: "results/{{ workflow.parameters.image_id }}/sbom"
        source: |
          set -e
          if [ "${IS_SCAN}" != "true" ]; then
            echo "Dependency Track scan is disabled"
            exit 0
          fi
          bomPath="/clusterscanner/data/bom.json"
          exportFilePath="/clusterscanner/data/findings.json"
          if [ "${DEPENDENCY_TRACK_KEY}" == "" ]; then
           echo "Error: DEPENDENCY_TRACK_KEY not set"
           exit 4
          fi
          bomBase64=$(cat "${bomPath}" | base64 -w 0)
          echo '{ "projectName": "'${PROJECT_NAME}'", "projectVersion": "'${VERSION}'", "autoCreate": true, "bom": "'${bomBase64}'" }' > /tmp/tmpfile
          curl -X "PUT" \
            "${DEPENDENCY_TRACK_URL}/api/v1/bom" \
            -H 'Content-Type: application/json' \
            -H "X-API-Key: ${DEPENDENCY_TRACK_KEY}" \
            -d @/tmp/tmpfile \
            --silent \
            --show-error \
            --output /tmp/token.json
          token=$(cat /tmp/token.json | jq -r '.token')

          counter=0
          processingPerformed="false"
          MAX_RETRIES=60
          while [ "${processingPerformed}" == "true" ]; do
           let counter=counter+1
           url="${DEPENDENCY_TRACK_URL}/api/v1/bom/token/${token}"
           echo "Fetching $url"
           processingPerformed=$(curl ${url} \
            --header 'Accept: application/json' \
            -H "X-API-Key: ${DEPENDENCY_TRACK_KEY}" \
            --silent \
            --show-error \
           | jq '.processing')
           echo "Processing status '${processingPerformed}' from Dependency Track"
           if [ ${counter} -gt ${MAX_RETRIES} ]; then
             echo "ERROR Reached maximum retries of ${MAX_RETRIES}, stop trying to get processing status"
             exit 2
           fi
           sleep 2
          done
          echo "GET ${DEPENDENCY_TRACK_URL}/api/v1/project?name=${PROJECT_NAME}&.."
          nameParameterUrlEncoded=$(printf %s "${PROJECT_NAME}" | jq -sRr @uri)
          curl "${DEPENDENCY_TRACK_URL}/api/v1/project?name=${nameParameterUrlEncoded}" \
           --silent \
           --show-error \
           --header 'Accept: application/json' \
           -H "X-API-Key: ${DEPENDENCY_TRACK_KEY}" \
           --output /tmp/project.json

          echo "/tmp/project.json"
          cat /tmp/project.json
          cat /tmp/project.json | jq -r '.[] | select(.version=="'${VERSION}'") | .uuid' > /tmp/project-uuid.txt

          export UUID=$(cat /tmp/project-uuid.txt)
          if [ "${UUID}" == "" ]; then
           echo "Error: UUID has the unexpceted value '${UUID}'"
           exit 1
          fi
          if [ $(echo "${UUID}" | wc -l) -ne 1 ]; then
           echo "Error: UUID '${UUID}' has unexpected newline"
           exit 3
          fi

          echo "Fetching ${DEPENDENCY_TRACK_URL}/api/v1/finding/project/${UUID}/export"

          exit 0
          ## In development: Check for new package version
          purl="^(?:(?!pkg:deb).)*$"
          purl="pkg:maven.*"
          purlEncodedValue=$(printf %s "${purl}" | jq -sRr @uri)
          curl -v "${DEPENDENCY_TRACK_URL}/api/v1/finding/project/${UUID}/export?purl=${purlEncodedValue}" \
           --silent \
           --show-error \
           -H "X-API-Key: ${DEPENDENCY_TRACK_KEY}" \
           --output ${exportFilePath}

          DEFAULT_NPM_BASE_URL="https://registry.npmjs.org";
          NPM_API_LATEST_URL="/-/package/%s/dist-tags";
          # http://localhost:8071/api/v1/component/project/bc33ac48-fb26-4142-99c3-96fdb0f07066?searchText=&sortOrder=asc&pageSize=100&pageNumber=1
          # jq | '.[]'
          # foreach
          if  [ repositoryType == "NPM" ]; then
            packageName="abab"
            version="1.0.0"

            packageNameEncoded=$(printf %s "${packageName}" | jq -sRr @uri)

            times=$(curl --silent  "${DEFAULT_NPM_BASE_URL}/${packageNameEncoded}" | jq -r ".time")
            next=false;
            for listedVersion in $versions; do
              if [ "${next}" == "true" ];then
                break;
              fi;
              #echo "${version} == ${listedVersion}";
              if [ "${version}" == "${listedVersion}" ]; then
                next="true";
              fi
            done

            publishedDate=$(curl --silent  "${DEFAULT_NPM_BASE_URL}/${packageNameEncoded}" | jq -r ".time[\"${listedVersion}\"]")

          fi


    - name: results-dependency-track-dd-upload
      volumes:
        - name: scandata
          persistentVolumeClaim:
            claimName: cluster-image-scanner-scandata
      outputs:
        artifacts:
          - name: results-dd-dependency-track-test-link
            path: /code/defectDojoTestLink.txt
          - name: is-finding-file
            path: /code/isFinding
          - name: findings-file
            path: /code/findings.json
      container:
        image: "{{ workflow.parameters.defectDojoClientImageName }}"
        volumeMounts:
          - name: scandata
            mountPath: /tmp/dependency-track-results
            subPath: "results/{{ workflow.parameters.image_id }}/sbom"
            readOnly: true
        imagePullPolicy: Always # TODO IfNotPresent
        retryStrategy:
          limit: 10
          retryPolicy: "OnFailure"
        command: [/usr/bin/groovy]
        args:
          - "/code/defectdojo.groovy"
        workingDir: "/code"
        envFrom:
          - configMapRef:
              name: "{{ workflow.parameters.DEFECTDOJO_CM }}"
          - configMapRef:
              name: "{{ workflow.parameters.scanjobEnvParameter }}"
          - secretRef:
              name: "{{ workflow.parameters.DEFECTDOJO_SECRETS }}"
        env:
          - name: DD_PRODUCT_NAME
            value: "{{ workflow.parameters.environment }} | {{ workflow.parameters.namespace }} | {{ workflow.parameters.appname }}"
          - name: DD_BRANCH_NAME
            value: "{{ workflow.parameters.image }}"
          - name: DD_REPORT_TYPE
            value: "Dependency Track Finding Packaging Format (FPF) Export"
          - name: NAMESPACE
            value: "{{ workflow.parameters.namespace }}"
          - name: ENVIRONMENT
            value: "{{ workflow.parameters.environment }}"
          - name: DD_TEAM
            value: "{{ workflow.parameters.team }}"
          - name: DD_REPORT_PATH
            value: "/tmp/dependency-track-results/findings.json"
          - name: EXIT_CODE_ON_FINDING
            value: "0"
          - name: EXIT_CODE_ON_MISSING_REPORT
            value: "0" # make the job successful
          - name: DD_MINIMUM_SEVERITY
            value: "High"


    - name: scan-distroless
      volumes:
        - name: images
          persistentVolumeClaim:
            claimName: cluster-image-scanner-images
            readOnly: true
        - name: scandata
          persistentVolumeClaim:
            claimName: cluster-image-scanner-scandata
      outputs:
        artifacts:
          - name: results-distroless
            path: /clusterscanner/data
            archiveLogs: true
      container:
        image: "{{ workflow.parameters.scanDistrolessImageName }}"
        command: ["/clusterscanner/entrypoint.bash"]
        imagePullPolicy: Always
        volumeMounts:
          - name: images
            mountPath: /clusterscanner/images
            subPath: "{{ workflow.parameters.image_id }}"
            readOnly: true
          - name: scandata
            mountPath: /clusterscanner/data
            subPath: "results/{{ workflow.parameters.image_id }}/generic/distroless"
        env:
          - name: IS_SCAN
            value: "{{ workflow.parameters.is_scan_distroless }}"
          - name: IMAGE_BY_HASH
            value: "{{ workflow.parameters.image_id }}"
        envFrom:
          - configMapRef:
              name: "{{ workflow.parameters.scanjobEnvParameter }}"

    - name: scan-lifetime
      volumes:
        - name: scandata
          persistentVolumeClaim:
            claimName: cluster-image-scanner-scandata
        - name: registry-creds
          secret:
            secretName: "{{ workflow.parameters.REGISTRY_SECRET }}"
      outputs:
        artifacts:
          - name: results-lifetime
            path: /clusterscanner/data
            archiveLogs: true
      container:
        image: "{{ workflow.parameters.scanLifetimeImageName }}"
        imagePullPolicy: Always
        command: ["/clusterscanner/entrypoint.bash"]
        volumeMounts:
          - name: scandata
            mountPath: /clusterscanner/data
            subPath: "results/{{ workflow.parameters.image_id }}/generic/lifetime"
          - name: registry-creds
            mountPath: /run/containers/auth.json
            subPath: auth.json
            defaultMode: 0444
        env:
          - name: IMAGE_BY_HASH
            value: "{{ workflow.parameters.image_id }}"
          - name: MAX_IMAGE_LIFETIME_IN_DAYS
            value: "{{ workflow.parameters.scan_lifetime_max_days }}"
          - name: IS_SCAN
            value: "{{ workflow.parameters.is_scan_lifetime }}"
        envFrom:
          - configMapRef:
              name: "{{ workflow.parameters.scanjobEnvParameter }}"

    - name: scan-baseimage-lifetime
      volumes:
        - name: scandata
          persistentVolumeClaim:
            claimName: cluster-image-scanner-scandata
        - name: registry-creds
          secret:
            secretName: "{{ workflow.parameters.REGISTRY_SECRET }}"
      outputs:
        artifacts:
          - name: results-baseimage-lifetime
            path: /clusterscanner/data
            archiveLogs: true
      container:
        image: "{{ workflow.parameters.scanLifetimeImageName }}"
        imagePullPolicy: Always
        command: ["/clusterscanner/entrypoint.bash"]
        volumeMounts:
          - name: scandata
            mountPath: /clusterscanner/data
            subPath: "results/{{ workflow.parameters.image_id }}/generic/baseimage-lifetime"
          - name: registry-creds
            mountPath: /run/containers/auth.json
            subPath: auth.json
            defaultMode: 0444
        env:
          - name: IMAGE_BY_HASH
            value: "{{ workflow.parameters.image_id }}"
          - name: MAX_IMAGE_LIFETIME_IN_DAYS
            value: "{{ workflow.parameters.scan_lifetime_max_days }}"
          - name: IS_SCAN
            value: "{{ workflow.parameters.is_scan_baseimage_lifetime }}"
          - name: MODULE_NAME
            value: "scan-baseimage-lifetime"
          - name: IS_BASE_IMAGE_LIFETIME_SCAN
            value: "true"
        envFrom:
          - configMapRef:
              name: "{{ workflow.parameters.scanjobEnvParameter }}"

    - name: scan-runasroot
      volumes:
        - name: registry-creds
          secret:
            secretName: "{{ workflow.parameters.REGISTRY_SECRET }}"
        - name: scandata
          persistentVolumeClaim:
            claimName: cluster-image-scanner-scandata
      outputs:
        artifacts:
          - name: results-runasroot
            path: /clusterscanner/data
            archiveLogs: true
      container:
        image: "{{ workflow.parameters.scanRootImageName }}"
        imagePullPolicy: IfNotPresent
        command: ["/clusterscanner/entrypoint.bash"]
        volumeMounts:
          - name: scandata
            mountPath: /clusterscanner/data
            subPath: "results/{{ workflow.parameters.image_id }}/generic/runasroot"
          - name: registry-creds
            mountPath: /run/containers/auth.json
            subPath: auth.json
            defaultMode: 0444
        env:
          - name: IMAGE_BY_HASH
            value: "{{ workflow.parameters.image_id }}"
          - name: IS_SCAN
            value: "{{ workflow.parameters.is_scan_runasroot }}"
        envFrom:
          - configMapRef:
              name: "{{ workflow.parameters.scanjobEnvParameter }}"

    - name: scan-new-version
      volumes:
        - name: registry-creds
          secret:
            secretName: "{{ workflow.parameters.REGISTRY_SECRET }}"
        - name: scandata
          persistentVolumeClaim:
            claimName: cluster-image-scanner-scandata
      outputs:
        artifacts:
          - name: results-runasroot
            path: /clusterscanner/data
            archiveLogs: true
      container:
        image: "{{ workflow.parameters.scanNewVersionImageName }}"
        imagePullPolicy: Always
        command: ["/clusterscanner/entrypoint.bash"]
        volumeMounts:
          - name: scandata
            mountPath: /clusterscanner/data
            subPath: "results/{{ workflow.parameters.image_id }}/generic/new-version"
          - name: registry-creds
            mountPath: /run/containers/auth.json
            subPath: auth.json
            defaultMode: 0444
        env:
          - name: IMAGE_BY_HASH
            value: "{{ workflow.parameters.image_id }}"
          - name: IMAGE
            value: "{{ workflow.parameters.image }}"
          - name: IS_SCAN
            value: "{{ workflow.parameters.is_scan_new_version }}"
          - name: IMAGE_SCAN_POSITIVE_FILTER
            value: "{{ workflow.parameters.new_version_image_filter }}"
        envFrom:
          - configMapRef:
              name: "{{ workflow.parameters.scanjobEnvParameter }}"

    - name: scan-malware
      volumes:
        - name: images
          persistentVolumeClaim:
            claimName: cluster-image-scanner-images
            readOnly: true
        - name: registry-creds
          secret:
            secretName: "{{ workflow.parameters.REGISTRY_SECRET }}"
        - name: scandata
          persistentVolumeClaim:
            claimName: cluster-image-scanner-scandata
      outputs:
        artifacts:
          - name: results-runasroot
            path: /clusterscanner/data
            archiveLogs: true
      container:
        image: "{{ workflow.parameters.scanMalwareImageName }}"
        imagePullPolicy: Always
        command: ["/clusterscanner/entrypoint.bash"]
        volumeMounts:
          - name: images
            mountPath: /clusterscanner/images
            subPath: "{{ workflow.parameters.image_id }}"
            readOnly: true
          - name: scandata
            mountPath: /clusterscanner/data
            subPath: "results/{{ workflow.parameters.image_id }}/generic/malware"
          - name: registry-creds
            mountPath: /run/containers/auth.json
            subPath: auth.json
            defaultMode: 0444
        env:
          - name: IMAGE_BY_HASH
            value: "{{ workflow.parameters.image_id }}"
          - name: IMAGE
            value: "{{ workflow.parameters.image }}"
          - name: IS_SCAN
            value: "{{ workflow.parameters.is_scan_malware }}"
        envFrom:
          - configMapRef:
              name: "{{ workflow.parameters.scanjobEnvParameter }}"

    - name: scan-dependency-check
      volumes:
        - name: images
          persistentVolumeClaim:
            claimName: cluster-image-scanner-images
            readOnly: true
        - name: scandata
          persistentVolumeClaim:
            claimName: cluster-image-scanner-scandata
        - name: suppressions
          configMap:
            name: "{{ workflow.parameters.dependencyCheckSuppressionsConfigMapName }}"
      outputs:
        artifacts:
          - name: results-dependency-check
            path: /clusterscanner/data/module_scan-dependency-check.json
            archiveLogs: true
          - name: results-dependency-check-report
            path: /clusterscanner/data
      container:
        image: "{{ workflow.parameters.scanDependencyCheckImageName }}"
        imagePullPolicy: Always # TODO
        command: ["/clusterscanner/entrypoint.bash"]
        volumeMounts:
          - name: images
            mountPath: /clusterscanner/images
            subPath: "{{ workflow.parameters.image_id }}"
            readOnly: true
          - name: scandata
            mountPath: /clusterscanner/data
            subPath: "results/{{ workflow.parameters.image_id }}/dependency-check"
          - name: suppressions
            mountPath: /tmp/suppressions
        env:
          - name: IMAGE_BY_HASH
            value: "{{ workflow.parameters.image_id }}"
          - name: IS_SCAN
            value: "{{ workflow.parameters.is_scan_dependency_check }}"
        envFrom:
          - configMapRef:
              name: "{{ workflow.parameters.DEPENDENCY_SCAN_CM }}"
          - configMapRef:
              name: "{{ workflow.parameters.scanjobEnvParameter }}"

    - name: results-dependency-check-dd-upload
      volumes:
        - name: scandata
          persistentVolumeClaim:
            claimName: cluster-image-scanner-scandata
      outputs:
        artifacts:
          - name: results-dd-dependency-check-test-link
            path: /code/defectDojoTestLink.txt
          - name: is-finding-file
            path: /code/isFinding
          - name: findings-file
            path: /code/findings.json
      container:
        image: "{{ workflow.parameters.defectDojoClientImageName }}"
        volumeMounts:
          - name: scandata
            mountPath: /tmp/dependency-check-results
            subPath: "results/{{ workflow.parameters.image_id }}/dependency-check"
            readOnly: true
        imagePullPolicy: Always # TODO IfNotPresent
        retryStrategy:
          limit: 10
          retryPolicy: "OnFailure"
        command: [/usr/bin/groovy]
        args:
          - "/code/defectdojo.groovy"
        workingDir: "/code"
        envFrom:
          - configMapRef:
              name: "{{ workflow.parameters.DEFECTDOJO_CM }}"
          - configMapRef:
              name: "{{ workflow.parameters.scanjobEnvParameter }}"
          - secretRef:
              name: "{{ workflow.parameters.DEFECTDOJO_SECRETS }}"
        env:
          - name: DD_PRODUCT_NAME
            value: "{{ workflow.parameters.environment }} | {{ workflow.parameters.namespace }} | {{ workflow.parameters.appname }}"
          - name: DD_BRANCH_NAME
            value: "{{ workflow.parameters.image }}"
          - name: DD_REPORT_TYPE
            value: "Dependency Check Scan"
          - name: NAMESPACE
            value: "{{ workflow.parameters.namespace }}"
          - name: ENVIRONMENT
            value: "{{ workflow.parameters.environment }}"
          - name: DD_TEAM
            value: "{{ workflow.parameters.team }}"
          - name: EXIT_CODE_ON_FINDING
            value: "0"
          - name: EXIT_CODE_ON_MISSING_REPORT
            value: "0" # make the job successful

    - name: results-collect-generic-findings
      volumes:
        - name: scandata
          persistentVolumeClaim:
            claimName: cluster-image-scanner-scandata
            readOnly: true
      outputs:
        artifacts:
          - name: results-generic
            path: /tmp/result.json
          - name: results-generic-findings
            path: /tmp/findings.csv
      script:
        image: "{{ workflow.parameters.baseImageName }}"
        volumeMounts:
          - name: scandata
            mountPath: /tmp/results
            subPath: "results/{{ workflow.parameters.image_id }}/generic"
        command: [/bin/bash]
        envFrom:
          - configMapRef:
              name: "{{ workflow.parameters.scanjobEnvParameter }}"
        source: |
          set -e
          JSON_RESULT=$(echo "{}" | jq -Sc '.+= {"scanResults": []}')
          find /tmp/results
          echo 'Date,Title,CweId,Url,Severity,Description,Mitigation,Impact,References,Active,Verified' > /tmp/findings.csv
          for j in /tmp/results/**/*.json; do
            echo "Collecting ${j}"
            JSON_RESULT=$(cat "${j}" | jq -ScM .)
            echo "${JSON_RESULT}"
            isFinding=$(echo "${JSON_RESULT}" | jq -ScM ".[].finding") || isFinding="false"
            if [ "${isFinding}" == "true" ]; then
              echo "Checking for csv:"
              ls -la $(dirname "${j}")/*.csv
              cat $(dirname "${j}")/*.csv >> /tmp/findings.csv || echo "No generic findinigs via csv given for ${j}"
            fi
            JSON_RESULTS=$(echo "${JSON_RESULT}" | jq -ScM ".scanResults += [${JSON_RESULT}]")
          done
          echo "${JSON_RESULTS}" > /tmp/result.json
          DATE=$(date +%m/%d/%Y)
          echo "Will replace placeholder IMAGE {{ workflow.parameters.image }} in /tmp/findings.csv and /tmp/result.json"
          sed -i "s\###IMAGE###\{{ workflow.parameters.image }}\g" /tmp/findings.csv
          sed -i "s\###IMAGE###\{{ workflow.parameters.image }}\g" /tmp/result.json
          echo "Will replace placeholder CLUSTER in /tmp/findings.csv and /tmp/result.json"
          sed -i "s/###CLUSTER###/{{ workflow.parameters.environment }}/g" /tmp/findings.csv
          sed -i "s/###CLUSTER###/{{ workflow.parameters.environment }}/g" /tmp/result.json
          echo "Will replace placeholder NAMESPACE in /tmp/findings.csv and /tmp/result.json"
          sed -i "s/###NAMESPACE###/{{ workflow.parameters.namespace }}/g" /tmp/findings.csv
          sed -i "s/###NAMESPACE###/{{ workflow.parameters.namespace }}/g" /tmp/result.json
          sed -i "s-###DATE###-$DATE-g" /tmp/findings.csv
          sed -i "s-###DATE###-$DATE-g" /tmp/result.json

    - name: results-generic-dd-upload
      inputs:
        artifacts:
          - name: results-generic-findings
            path: /tmp/generic-results.csv
      outputs:
        artifacts:
          - name: results-dd-generic-test-link
            path: /code/defectDojoTestLink.txt
          - name: is-finding-file
            path: /code/isFinding
          - name: findings-file
            path: /code/findings.json
      container:
        image: "{{ workflow.parameters.defectDojoClientImageName }}"
        imagePullPolicy: IfNotPresent
        retryStrategy:
          limit: 10
          retryPolicy: "OnFailure"
        command: [/usr/bin/groovy]
        args:
          - "/code/defectdojo.groovy"
        workingDir: "/code"
        envFrom:
          - configMapRef:
              name: "{{ workflow.parameters.DEFECTDOJO_CM }}"
          - configMapRef:
              name: "{{ workflow.parameters.scanjobEnvParameter }}"
          - secretRef:
              name: "{{ workflow.parameters.DEFECTDOJO_SECRETS }}"
        env:
          - name: DD_PRODUCT_NAME
            value: "{{ workflow.parameters.environment }} | {{ workflow.parameters.namespace }} | {{ workflow.parameters.appname }}"
          - name: DD_BRANCH_NAME
            value: "{{ workflow.parameters.image }}"
          - name: DD_REPORT_TYPE
            value: "Generic Findings Import"
          - name: DD_REPORT_PATH
            value: /tmp/generic-results.csv
          - name: EXIT_CODE_ON_FINDING
            value: "0"
          - name: NAMESPACE
            value: "{{ workflow.parameters.namespace }}"
          - name: ENVIRONMENT
            value: "{{ workflow.parameters.environment }}"
          - name: DD_TEAM
            value: "{{ workflow.parameters.team }}"
          - name: EXIT_CODE_ON_MISSING_REPORT
            value: "0" # make the job successful
          - name: DD_MINIMUM_SEVERITY
            value: "Medium"

    - name: results-aggregate
      inputs:
        artifacts:
          - name: results-dd-generic-test-link
            path: /tmp/dd-generic-test-link.txt
          - name: results-dd-generic-is-finding-file
            path: /tmp/isFinding-generic
          - name: results-dd-generic-findings-file
            path: /tmp/findings-generic.json
          - name: results-generic
            path: /tmp/result.json
          - name: results-dd-dependency-check-test-link
            path: /tmp/dd-dependency-check-test-link.txt
          - name: results-dd-dependency-check-is-finding-file
            path: /tmp/isFinding-dependency-check
          - name: results-dd-dependency-check-findings-file
            path: /tmp/findings-dependency-check.json
          - name: results-dd-dependency-track-test-link
            path: /tmp/dd-dependency-track-test-link.txt
          - name: results-dd-dependency-track-is-finding-file
            path: /tmp/isFinding-dependency-track
          - name: results-dd-dependency-track-findings-file
            path: /tmp/findings-dependency-track.json
        parameters:
          - name: dd-generic-exitcode
          - name: dd-generic-startedat
          - name: dd-generic-finishedat
          - name: dd-dependency-check-exitcode
          - name: dd-dependency-check-startedat
          - name: dd-dependency-check-finishedat
          - name: dd-dependency-track-exitcode
          - name: dd-dependency-track-startedat
          - name: dd-dependency-track-finishedat
      volumes:
        - name: scandata
          persistentVolumeClaim:
            claimName: cluster-image-scanner-scandata
      script:
        image: "{{ workflow.parameters.baseImageName }}"
        volumeMounts:
          - name: scandata
            mountPath: /clusterscanner/data
            subPath: "{{ workflow.parameters.SCAN_ID }}"
            defaultMode: 0777 # to allow the cleanup to delete the folders
        command: [/bin/bash]
        envFrom:
          - configMapRef:
              name: "{{ workflow.parameters.scanjobEnvParameter }}"
        source: |
          set -e
          IMAGE_NAME=$(echo "{{ workflow.parameters.image }}" | cut -d: -f1)
          IMAGE_NAME_CLEANED=$(echo "${IMAGE_NAME}" | sed -e "s#/#__#g")
          IMAGE_TAG=$(echo "{{ workflow.parameters.image }}" | cut -d: -f2)
          IMAGE_HASH=$(echo "{{ workflow.parameters.image_id }}" | cut -d: -f2)

          JSON_RESULT=$(cat /tmp/result.json)
          JSON_RESULT=$(echo "${JSON_RESULT}" | jq -ScM ".imageTag = \"${IMAGE_TAG}\"")
          JSON_RESULT=$(echo "${JSON_RESULT}" | jq -ScM ".imageHash = \"${IMAGE_HASH}\"")
          JSON_RESULT=$(echo "${JSON_RESULT}" | jq -ScM '.image = "{{ workflow.parameters.image }}"')
          JSON_RESULT=$(echo "${JSON_RESULT}" | jq -ScM '.environment = "{{ workflow.parameters.environment }}"')
          JSON_RESULT=$(echo "${JSON_RESULT}" | jq -ScM '.namespace = "{{ workflow.parameters.namespace }}"')
          JSON_RESULT=$(echo "${JSON_RESULT}" | jq -ScM '.scm_source_branch = "{{ workflow.parameters.scm_source_branch }}"')
          JSON_RESULT=$(echo "${JSON_RESULT}" | jq -ScM '.team = "{{ workflow.parameters.team }}"')
          JSON_RESULT=$(echo "${JSON_RESULT}" | jq -ScM '.slack = "{{ workflow.parameters.slack }}"')
          JSON_RESULT=$(echo "${JSON_RESULT}" | jq -ScM '.email = "{{ workflow.parameters.email }}"')
          JSON_RESULT=$(echo "${JSON_RESULT}" | jq -ScM '.appname = "{{ workflow.parameters.appname }}"')

          JSON_DD_DEPENDENCY_TRACK=$(echo "{\"errors\":[]}" | jq -Sc ".+= {\"startedAt\": \"{{ inputs.parameters.dd-dependency-track-startedat }}\"}")
          isFinding=$(cat /tmp/isFinding-dependency-track) || isFinding="false"
          if [ "$isFinding" == "true" ]; then
            DD_LINK=$(cat /tmp/dd-dependency-track-test-link.txt)
            JSON_DD_DEPENDENCY_TRACK=$(echo ${JSON_DD_DEPENDENCY_TRACK} | jq -Sc ". += {\"status\": \"completed\", \"finding\": true, \"infoText\": \"Potential unhandled known vulnerabilities found in image\", \"ddLink\": \"${DD_LINK}\"}")
            if [ $(cat /tmp/findings-dependency-track.json | wc -c) -gt 5 ]; then
              echo "Starting DepCheck Aggregate"
              echo $JSON_DD_DEPENDENCY_TRACK > /tmp/dd-deptrack.json
              FILESIZE=$(stat -c%s "/tmp/findings-dependency-track.json")
              FILESIZE_LIMT=25600 # 20kb
              if [ $FILESIZE -lt $FILESIZE_LIMT ]; then
                echo "Filesize of $FILESIZE is lt $FILESIZE_LIMT for /tmp/findings-dependency-track.json"
                JSON_DD_DEPENDENCY_TRACK=$(jq '.findings += input'  /tmp/dd-deptrack.json /tmp/findings-dependency-track.json)
              else
                echo "Filesize of $FILESIZE is gt $FILESIZE_LIMT /tmp/findings-dependency-track.json"
                echo '[{ "title": "Dependency Track Finding", "description": "OWASP Dependency Track found some findings, please check DefectDojo" }]' > /tmp/findings-dependency-track-zero.json
                JSON_DD_DEPENDENCY_TRACK=$(jq '.findings += input'  /tmp/dd-deptrack.json /tmp/findings-dependency-track-zero.json)
              fi
            else
              JSON_DD_DEPENDENCY_TRACK=$(echo ${JSON_DD_DEPENDENCY_TRACK} | jq -Sc ". += {\"findings\": []}")
              echo "Warning: /tmp/findings-dependency-track.json is empty"
            fi
          elif [ "xX{{ inputs.parameters.dd-dependency-track-exitcode }}" != "xX0" ]; then
            echo "Exit code != 0"
            JSON_DD_DEPENDENCY_TRACK=$(echo ${JSON_DD_DEPENDENCY_TRACK} | jq -Sc ". += {\"status\": \"failed\", \"finding\": false, \"infoText\": \"Uploading report to DefectDojo failed.\", \"findings\": []}")
          else
            JSON_DD_DEPENDENCY_TRACK=$(echo ${JSON_DD_DEPENDENCY_TRACK} | jq -Sc ". += {\"status\": \"completed\", \"finding\": false, \"ddLink\": \"${DD_LINK}\", \"findings\": []}")
          fi
          JSON_DD_DEPENDENCY_TRACK=$(echo ${JSON_DD_DEPENDENCY_TRACK} | jq -Sc ". += {\"finishedAt\": \"{{ inputs.parameters.dd-dependency-track-finishedat }}\"}")
          JSON_DD_DEPENDENCY_TRACK=$(echo ${JSON_DD_DEPENDENCY_TRACK} | jq -Sc ". += {\"scanType\": \"Dependency Track\"}")



          JSON_DD_DEPENDENCY_CHECK=$(echo "{\"errors\":[]}" | jq -Sc ".+= {\"startedAt\": \"{{ inputs.parameters.dd-dependency-check-startedat }}\"}")
          isFinding=$(cat /tmp/isFinding-dependency-check) || isFinding="false"
          if [ "$isFinding" == "true" ]; then
            DD_LINK=$(cat /tmp/dd-dependency-check-test-link.txt)
            JSON_DD_DEPENDENCY_CHECK=$(echo ${JSON_DD_DEPENDENCY_CHECK} | jq -Sc ". += {\"status\": \"completed\", \"finding\": true, \"infoText\": \"Potential unhandled known vulnerabilities found in image\", \"ddLink\": \"${DD_LINK}\"}")
            if [ $(cat /tmp/findings-dependency-check.json | wc -c) -gt 5 ]; then
              echo "Starting DepCheck Aggregate"
              echo $JSON_DD_DEPENDENCY_CHECK > /tmp/dd-depcheck.json
              FILESIZE=$(stat -c%s "/tmp/findings-dependency-check.json")
              FILESIZE_LIMT=25600 # 20kb
              if [ $FILESIZE -lt $FILESIZE_LIMT ]; then
                echo "Filesize of $FILESIZE is lt $FILESIZE_LIMT"
                JSON_DD_DEPENDENCY_CHECK=$(jq '.findings += input'  /tmp/dd-depcheck.json /tmp/findings-dependency-check.json)
              else
                echo "Filesize of $FILESIZE is gt $FILESIZE_LIMT"
                echo '[{ "title": "Dependency Check Finding", "description": "OWASP Dependency Check found some findings, please check DefectDojo" }]' > /tmp/findings-dependency-check-zero.json
                JSON_DD_DEPENDENCY_CHECK=$(jq '.findings += input'  /tmp/dd-depcheck.json /tmp/findings-dependency-check-zero.json)
              fi
            else
              JSON_DD_DEPENDENCY_CHECK=$(echo ${JSON_DD_DEPENDENCY_CHECK} | jq -Sc ". += {\"findings\": []}")
              echo "Warning: /tmp/findings-dependency-check.json is empty"
            fi
          elif [ "xX{{ inputs.parameters.dd-dependency-check-exitcode }}" != "xX0" ]; then
            echo "Exit code != 0"
            JSON_DD_DEPENDENCY_CHECK=$(echo ${JSON_DD_DEPENDENCY_CHECK} | jq -Sc ". += {\"status\": \"failed\", \"finding\": false, \"infoText\": \"Uploading report to DefectDojo failed.\", \"findings\": []}")
          else
            JSON_DD_DEPENDENCY_CHECK=$(echo ${JSON_DD_DEPENDENCY_CHECK} | jq -Sc ". += {\"status\": \"completed\", \"finding\": false, \"ddLink\": \"${DD_LINK}\", \"findings\": []}")
          fi
          JSON_DD_DEPENDENCY_CHECK=$(echo ${JSON_DD_DEPENDENCY_CHECK} | jq -Sc ". += {\"finishedAt\": \"{{ inputs.parameters.dd-dependency-check-finishedat }}\"}")
          JSON_DD_DEPENDENCY_CHECK=$(echo ${JSON_DD_DEPENDENCY_CHECK} | jq -Sc ". += {\"scanType\": \"Dependency Check\"}")

          echo "Starting Generic"
          JSON_DD_GENERIC=$(echo "{\"errors\":[]}" | jq -Sc ".+= {\"startedAt\": \"{{ inputs.parameters.dd-generic-startedat }}\"}")
          isFinding=$(cat /tmp/isFinding-generic) || isFinding="false"
          if [ "$isFinding" == "true" ]; then
            DD_LINK=$(cat /tmp/dd-generic-test-link.txt)
            JSON_DD_GENERIC=$(echo ${JSON_DD_GENERIC} | jq -Sc ". += {\"status\": \"completed\", \"finding\": true, \"infoText\": \"Potential unhandled vulnerabilities or misconfigurations found in image\", \"ddLink\": \"${DD_LINK}\"}")
            if [ $(cat /tmp/findings-generic.json  | wc -c) -gt 5 ]; then
              echo $JSON_DD_GENERIC> /tmp/generic.json
              JSON_DD_GENERIC=$(jq '.findings += input'  /tmp/generic.json /tmp/findings-generic.json)
            else
              JSON_DD_GENERIC=$(echo ${JSON_DD_DEPENDENCY_GENERIC} | jq -Sc ". += {\"findings\": []}")
              echo "Warning: /tmp/finding-generic  is empty"
            fi
          elif [ "xX{{ inputs.parameters.dd-generic-exitcode }}" != "xX0" ]; then
            JSON_DD_GENERIC=$(echo ${JSON_DD_GENERIC} | jq -Sc ". += {\"status\": \"failed\", \"finding\": false, \"infoText\": \"Uploading report to DefectDojo failed.\"}")
          else
            JSON_DD_GENERIC=$(echo ${JSON_DD_GENERIC} | jq -Sc ". += {\"status\": \"completed\", \"finding\": false, \"ddLink\": \"${DD_LINK}\", \"findings\": []}")
          fi
          JSON_DD_GENERIC=$(echo ${JSON_DD_GENERIC} | jq -Sc ". += {\"finishedAt\": \"{{ inputs.parameters.dd-generic-finishedat }}\"}")
          JSON_DD_GENERIC=$(echo ${JSON_DD_GENERIC} | jq -Sc ". += {\"scanType\": \"Generic\"}")

          uploadResults="[{\"ddGenericUpload\": ${JSON_DD_GENERIC}}, {\"ddDependencyCheckUpload\": ${JSON_DD_DEPENDENCY_CHECK}}, {\"ddDependencyTrackUpload\": ${JSON_DD_DEPENDENCY_TRACK}}]"
          echo "uploadResults: ${uploadResults}"
          JSON_RESULT=$(echo "${JSON_RESULT}" | jq -ScM ".uploadResults += ${uploadResults}")
          DD_DEPENDENCY_CHECK_FINDING=$(echo "${JSON_RESULT}" | jq -ScM ".uploadResults[] | select(.ddDependencyCheckUpload) | .ddDependencyCheckUpload.finding")
          DD_DEPENDENCY_TRACK_FINDING=$(echo "${JSON_RESULT}" | jq -ScM ".uploadResults[] | select(.ddDependencyTrackUpload) | .ddDependencyTrackUpload.finding")
          DD_GENERIC_FINDING=$(echo "${JSON_RESULT}" | jq -ScM ".uploadResults[] | select(.ddGenericUpload) | .ddGenericUpload.finding")
          if [ "xX${DD_DEPENDENCY_CHECK_FINDING}" == "xXtrue" ] || [ "xX${DD_GENERIC_FINDING}" == "xXtrue" ] || [ "xX${DD_DEPENDENCY_TRACK_FINDING}" == "xXtrue" ]; then
            JSON_RESULT=$(echo "${JSON_RESULT}" | jq -ScM ".notificationRequired = true")
          else
            JSON_RESULT=$(echo "${JSON_RESULT}" | jq -ScM ".notificationRequired = false")
          fi

          echo "Storing into /clusterscanner/data/{{ workflow.parameters.environment }}__{{ workflow.parameters.namespace }}__${IMAGE_NAME_CLEANED}--${IMAGE_HASH}"
          echo "${JSON_RESULT}"
          echo "${JSON_RESULT}"  > /clusterscanner/data/{{ workflow.parameters.environment }}__{{ workflow.parameters.namespace }}__${IMAGE_NAME_CLEANED}--${IMAGE_HASH}.json

